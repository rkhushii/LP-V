# -*- coding: utf-8 -*-
"""ass2-ocr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qqrzh21yyzHEW3SRGzG1Vkrzt1O5KQtv
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import requests

url='https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data'
data=requests.get(url)
with open('letter-recognition.data','wb') as file:
    file.write(data.content)

data.head()

columns=['letter']+[f'f{i}' for i in range(1,17)]

# Load data and split into features and labels
data = pd.read_csv("letter-recognition.data", header=None,names=columns)
X = data[columns[1:]]  # Features (columns 1 to end)
y = data['letter']   # Labels (first column)

# Encode labels to 0â€“25
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

y_encoded

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(16,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.Dense(26, activation='softmax')  # 26 classes
])

model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)

# Plot Accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='o')
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Pick a sample from the validation set
sample = X_val.values[0].reshape(1, -1)  # Reshape to match model input shape

# Predict
prediction = model.predict(sample)
predicted_class_index = np.argmax(prediction)
predicted_letter = label_encoder.inverse_transform([predicted_class_index])

# Show result
print("Predicted Letter:", predicted_letter[0])
print("Actual Letter:", label_encoder.inverse_transform([y_val[0]])[0])